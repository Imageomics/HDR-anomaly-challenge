{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "score_dir = os.path.abspath('../scoring_program/')\n",
    "sys.path.append(score_dir)\n",
    "\n",
    "from score_combined import *\n",
    "\n",
    "def cmp_results(score_type, pred_dict, gt_dict):\n",
    "    WIDTH = 18\n",
    "    print(f'====== compare {score_type} {\"=\" * (WIDTH - len(score_type))}')\n",
    "    for k,v in gt_dict[score_type].items():\n",
    "        pred_value = pred_dict[k]\n",
    "        if pred_value == v:\n",
    "            print(f'{k} {\" \" * (WIDTH - len(k))} matched.')\n",
    "        else:\n",
    "            print(f'{k} {\" \" * (WIDTH - len(k))} should be {v}, but get {pred_value}.')\n",
    "    print('==================================\\n')\n",
    "\n",
    "def validate(root_dir, score_type='both'):\n",
    "    prediction_file = os.path.join(root_dir,'prediction.txt')\n",
    "    pred_df = pd.read_csv(prediction_file, header=None, names=['filename','preds'])\n",
    "\n",
    "    solution_file = os.path.join(root_dir,'reference.csv')\n",
    "    sol_df_A, sol_df_mimic = parse_solution_file(solution_file)\n",
    "\n",
    "    gt_file = os.path.join(root_dir,'gt.json')\n",
    "    with open(gt_file,'r') as load_f:\n",
    "        gt_dict = json.load(load_f)\n",
    "\n",
    "    if score_type == 'mimic_scores':\n",
    "        mimic_scores = get_scores(pred_df=pred_df, sol_df=sol_df_mimic, mm_vals = False)\n",
    "        cmp_results('mimic_scores', mimic_scores, gt_dict)\n",
    "        return mimic_scores\n",
    "    elif score_type == 'A_scores':\n",
    "        A_scores = get_scores(pred_df=pred_df, sol_df=sol_df_A, mm_vals = True)\n",
    "        cmp_results('A_scores', A_scores, gt_dict)\n",
    "        return A_scores\n",
    "    else:\n",
    "        mimic_scores = get_scores(pred_df=pred_df, sol_df=sol_df_mimic, mm_vals = False)\n",
    "        A_scores = get_scores(pred_df=pred_df, sol_df=sol_df_A, mm_vals = True)\n",
    "        cmp_results('mimic_scores', mimic_scores, gt_dict)\n",
    "        cmp_results('A_scores', A_scores, gt_dict)\n",
    "        return mimic_scores, A_scores\n",
    "\n",
    "def save_gt(root_dir, mimic_scores, A_scores):\n",
    "    gt_dict = {'mimic_scores': mimic_scores,'A_scores': A_scores}\n",
    "\n",
    "    with open(os.path.join(root_dir,'gt.json'),'w') as dump_f:\n",
    "        json.dump(gt_dict, dump_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0 are all non-hybrids and the ones higher are all hyrids.\n",
      "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.0, 'hybrid_recall': 1.0, 'hybrid_precision': 1.0, 'f1_score': 1.0, 'accuracy': 1.0, 'prc_auc': 1.0, 'roc_auc': 1.0}\n",
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0 are all non-hybrids and the ones higher are all hyrids.\n",
      "Evaluating performance on signal vs non-signal hybrids\n",
      "Full Scores Species A hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.0, 'hybrid_recall': 1.0, 'hybrid_precision': 1.0, 'f1_score': 1.0, 'accuracy': 1.0, 'prc_auc': 1.0, 'roc_auc': 1.0, 'major_recall': np.float64(1.0), 'minor_recall': np.float64(1.0), 'major_prc_auc': np.float64(1.0), 'minor_prc_auc': np.float64(1.0), 'major_roc_auc': np.float64(1.0), 'minor_roc_auc': np.float64(1.0)}\n",
      "====== compare mimic_scores ======\n",
      "threshold_recall    matched.\n",
      "threshold_pred      matched.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "==================================\n",
      "\n",
      "====== compare A_scores ==========\n",
      "threshold_recall    matched.\n",
      "threshold_pred      matched.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "major_recall        matched.\n",
      "minor_recall        matched.\n",
      "major_prc_auc       matched.\n",
      "minor_prc_auc       matched.\n",
      "major_roc_auc       matched.\n",
      "minor_roc_auc       matched.\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all the prediction are correct\n",
    "root_dir = 'all_correct'\n",
    "_,_ = validate(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 1 are all non-hybrids and the ones higher are all hyrids.\n",
      "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 1.0, 'hybrid_recall': 0.0, 'hybrid_precision': 0.0, 'f1_score': 0.0, 'accuracy': 0.5, 'prc_auc': 0.5, 'roc_auc': 0.0}\n",
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 1 are all non-hybrids and the ones higher are all hyrids.\n",
      "Evaluating performance on signal vs non-signal hybrids\n",
      "Full Scores Species A hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 1.0, 'hybrid_recall': 0.0, 'hybrid_precision': 0.0, 'f1_score': 0.0, 'accuracy': 0.5, 'prc_auc': 0.5, 'roc_auc': 0.0, 'major_recall': np.float64(0.0), 'minor_recall': np.float64(0.0), 'major_prc_auc': np.float64(0.5), 'minor_prc_auc': np.float64(0.5), 'major_roc_auc': np.float64(0.0), 'minor_roc_auc': np.float64(0.0)}\n",
      "====== compare mimic_scores ======\n",
      "threshold_recall    matched.\n",
      "threshold_pred      matched.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "==================================\n",
      "\n",
      "====== compare A_scores ==========\n",
      "threshold_recall    matched.\n",
      "threshold_pred      matched.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "major_recall        matched.\n",
      "minor_recall        matched.\n",
      "major_prc_auc       matched.\n",
      "minor_prc_auc       matched.\n",
      "major_roc_auc       matched.\n",
      "minor_roc_auc       matched.\n",
      "==================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wu.5686/miniconda3/envs/anomaly_train/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/wu.5686/miniconda3/envs/anomaly_train/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# all the predictions are wrong\n",
    "root_dir = 'all_wrong'\n",
    "_,_ = validate(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0 are all non-hybrids and the ones higher are all hyrids.\n",
      "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.0, 'hybrid_recall': 0.0, 'hybrid_precision': 0.0, 'f1_score': 0.0, 'accuracy': 0.5, 'prc_auc': 0.5, 'roc_auc': 0.5}\n",
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0 are all non-hybrids and the ones higher are all hyrids.\n",
      "Evaluating performance on signal vs non-signal hybrids\n",
      "Full Scores Species A hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.0, 'hybrid_recall': 0.0, 'hybrid_precision': 0.0, 'f1_score': 0.0, 'accuracy': 0.5, 'prc_auc': 0.5, 'roc_auc': 0.5, 'major_recall': np.float64(0.0), 'minor_recall': np.float64(0.0), 'major_prc_auc': np.float64(0.5), 'minor_prc_auc': np.float64(0.5), 'major_roc_auc': np.float64(0.5), 'minor_roc_auc': np.float64(0.5)}\n",
      "====== compare mimic_scores ======\n",
      "threshold_recall    matched.\n",
      "threshold_pred      matched.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "==================================\n",
      "\n",
      "====== compare A_scores ==========\n",
      "threshold_recall    matched.\n",
      "threshold_pred      matched.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "major_recall        matched.\n",
      "minor_recall        matched.\n",
      "major_prc_auc       matched.\n",
      "minor_prc_auc       matched.\n",
      "major_roc_auc       matched.\n",
      "minor_roc_auc       matched.\n",
      "==================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wu.5686/miniconda3/envs/anomaly_train/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/wu.5686/miniconda3/envs/anomaly_train/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# all the predictions are zeros\n",
    "root_dir = 'all_zeros'\n",
    "_,_ = validate(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0 are all non-hybrids and the ones higher are all hyrids.\n",
      "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.0, 'hybrid_recall': 1.0, 'hybrid_precision': 1.0, 'f1_score': 1.0, 'accuracy': 1.0, 'prc_auc': 1.0, 'roc_auc': 1.0}\n",
      "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0 are all non-hybrids and the ones higher are all hyrids.\n",
      "Evaluating performance on signal vs non-signal hybrids\n",
      "Full Scores Species A hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.0, 'hybrid_recall': 1.0, 'hybrid_precision': 1.0, 'f1_score': 1.0, 'accuracy': 1.0, 'prc_auc': 1.0, 'roc_auc': 1.0, 'major_recall': np.float64(1.0), 'minor_recall': np.float64(1.0), 'major_prc_auc': np.float64(1.0), 'minor_prc_auc': np.float64(1.0), 'major_roc_auc': np.float64(1.0), 'minor_roc_auc': np.float64(1.0)}\n",
      "====== compare mimic_scores ======\n",
      "threshold_recall    should be 3.0, but get 1.0.\n",
      "threshold_pred      should be 2.0, but get 0.0.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "==================================\n",
      "\n",
      "====== compare A_scores ==========\n",
      "threshold_recall    should be 2.0, but get 1.0.\n",
      "threshold_pred      should be 3.0, but get 0.0.\n",
      "hybrid_recall       matched.\n",
      "hybrid_precision    matched.\n",
      "f1_score            matched.\n",
      "accuracy            matched.\n",
      "prc_auc             matched.\n",
      "roc_auc             matched.\n",
      "major_recall        matched.\n",
      "minor_recall        matched.\n",
      "major_prc_auc       matched.\n",
      "minor_prc_auc       matched.\n",
      "major_roc_auc       matched.\n",
      "minor_roc_auc       matched.\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a sample of all the prediction are correct, but the gt is wrong\n",
    "root_dir = 'bad_gt'\n",
    "_,_ = validate(root_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
