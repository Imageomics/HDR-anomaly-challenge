{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validate Scoring Sript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vVDeIW_31n5T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, average_precision_score, roc_auc_score, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wY5htaVewhpe"
      },
      "outputs": [],
      "source": [
        "\n",
        "csv_path = \"res/Dummy_Ref_Data.csv"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mToomvMq3mR9"
      },
      "outputs": [],
      "source": [
        "def parse_solution_file(path):\n",
        "    # hybrid stat is the 0-1 indicator\n",
        "    df = pd.read_csv(path, dtype = {\"hybrid_stat\": np.int32})\n",
        "    # Get mimic dataframe\n",
        "    df_mimic = df.loc[df[\"ssp_indicator\"] == \"mimic\"].copy()\n",
        "    # Get Species A DataFrame and process it\n",
        "    df_A = df.loc[df[\"ssp_indicator\"] != \"mimic\"].copy()\n",
        "\n",
        "    return df_A, df_mimic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jomlzJUtwnRj",
        "outputId": "43e5e69a-8150-4d40-be67-849238ad1594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Species A DataFrame:\n",
            "   filename  hybrid_stat ssp_indicator\n",
            "0     file1            0         major\n",
            "1     file2            0         minor\n",
            "2     file3            1         minor\n",
            "3     file4            0         minor\n",
            "7     file8            0         minor\n",
            "8     file9            1         major\n",
            "10   file11            1         major\n",
            "12   file13            0         minor\n",
            "14   file15            1         minor\n",
            "16   file17            0         major\n",
            "17   file18            0         major\n",
            "21   file22            0         minor\n",
            "23   file24            0         major\n",
            "24   file25            0         major\n",
            "25   file26            1         major\n",
            "26   file27            1         minor\n",
            "27   file28            0         major\n",
            "28   file29            0         minor\n",
            "30   file31            0         major\n",
            "31   file32            0         major\n",
            "33   file34            1         major\n",
            "36   file37            0         minor\n",
            "37   file38            1         major\n",
            "38   file39            1         minor\n",
            "39   file40            1         minor\n",
            "40   file41            0         minor\n",
            "41   file42            1         minor\n",
            "43   file44            1         minor\n",
            "44   file45            0         major\n",
            "46   file47            1         major\n",
            "47   file48            1         minor\n",
            "49   file50            0         major\n",
            "\n",
            "Mimic DataFrame:\n",
            "   filename  hybrid_stat ssp_indicator\n",
            "4     file5            0         mimic\n",
            "5     file6            1         mimic\n",
            "6     file7            0         mimic\n",
            "9    file10            1         mimic\n",
            "11   file12            0         mimic\n",
            "13   file14            1         mimic\n",
            "15   file16            0         mimic\n",
            "18   file19            0         mimic\n",
            "19   file20            0         mimic\n",
            "20   file21            0         mimic\n",
            "22   file23            0         mimic\n",
            "29   file30            1         mimic\n",
            "32   file33            1         mimic\n",
            "34   file35            0         mimic\n",
            "35   file36            1         mimic\n",
            "42   file43            1         mimic\n",
            "45   file46            0         mimic\n",
            "48   file49            1         mimic\n"
          ]
        }
      ],
      "source": [
        "df_A, df_mimic = parse_solution_file(csv_path)\n",
        "\n",
        "print(\"Species A DataFrame:\")\n",
        "print(df_A)\n",
        "print(\"\\nMimic DataFrame:\")\n",
        "print(df_mimic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_DwOVTeB4Xoe"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"preds\": [0.1, 0.4, 0.35, 0.8, 0.5, 0.9, 0.2, 0.3, 0.7, 0.15, 0.6, 0.25, 0.55, 0.85, 0.65, 0.45, 0.05, 0.75, 0.95, 0.33],\n",
        "    \"hybrid_stat\": [0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1],\n",
        "    \"ssp_indicator\": [\"major\", \"major\", \"mimic\", \"minor\", \"major\", \"mimic\", \"minor\", \"mimic\", \"minor\", \"major\", \"mimic\", \"major\", \"minor\", \"mimic\", \"minor\", \"major\", \"minor\", \"minor\", \"major\", \"mimic\"],\n",
        "    \"pre_converted_preds\": [0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0]\n",
        "}\n",
        "\n",
        "score_df = pd.DataFrame(data)\n",
        "# 0.55 threshold\n",
        "# Recall = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tCRJj8HP3p8c"
      },
      "outputs": [],
      "source": [
        "def evaluate_prediction(score_df):\n",
        "    # to get a new score_df with converted_pred\n",
        "    # loop predictions from most likely non-hybrid to most likely hybrid\n",
        "    for threshold_pred in sorted(set(score_df[\"preds\"])):\n",
        "        score_df['converted_preds'] = score_df[\"preds\"].apply(lambda x: 1 if x > threshold_pred else 0)\n",
        "\n",
        "        threshold_recall = recall_score(score_df[\"hybrid_stat\"], score_df[\"converted_preds\"], pos_label=0)  # non-hybrid is the positive here, so positive label is 0\n",
        "\n",
        "        print(f\"Threshold: {threshold_pred}, Recall: {threshold_recall}\")\n",
        "\n",
        "        if threshold_recall >= 0.95:\n",
        "            break\n",
        "\n",
        "    print(f'With non-hybrid recall {str(round(threshold_recall, 4))}, the predictions equal and lower than the threshold confident score {str(threshold_pred)} are all non-hybrids and the ones higher are all hybrids.')\n",
        "\n",
        "    return score_df, threshold_recall, threshold_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCS7UDsF4Z13",
        "outputId": "9bfbdb30-1244-4207-a3c9-765c451879d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold: 0.05, Recall: 0.1111111111111111\n",
            "Threshold: 0.1, Recall: 0.2222222222222222\n",
            "Threshold: 0.15, Recall: 0.3333333333333333\n",
            "Threshold: 0.2, Recall: 0.4444444444444444\n",
            "Threshold: 0.25, Recall: 0.5555555555555556\n",
            "Threshold: 0.3, Recall: 0.5555555555555556\n",
            "Threshold: 0.33, Recall: 0.5555555555555556\n",
            "Threshold: 0.35, Recall: 0.5555555555555556\n",
            "Threshold: 0.4, Recall: 0.6666666666666666\n",
            "Threshold: 0.45, Recall: 0.7777777777777778\n",
            "Threshold: 0.5, Recall: 0.8888888888888888\n",
            "Threshold: 0.55, Recall: 1.0\n",
            "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0.55 are all non-hybrids and the ones higher are all hybrids.\n",
            "\n",
            "Modified DataFrame:\n",
            "    preds  hybrid_stat ssp_indicator  pre_converted_preds  converted_preds\n",
            "0    0.10            0         major                    0                0\n",
            "1    0.40            0         major                    0                0\n",
            "2    0.35            1         mimic                    0                0\n",
            "3    0.80            1         minor                    1                1\n",
            "4    0.50            0         major                    0                0\n",
            "5    0.90            1         mimic                    1                1\n",
            "6    0.20            0         minor                    0                0\n",
            "7    0.30            1         mimic                    0                0\n",
            "8    0.70            1         minor                    1                1\n",
            "9    0.15            0         major                    0                0\n",
            "10   0.60            1         mimic                    1                1\n",
            "11   0.25            0         major                    0                0\n",
            "12   0.55            0         minor                    0                0\n",
            "13   0.85            1         mimic                    1                1\n",
            "14   0.65            1         minor                    1                1\n",
            "15   0.45            0         major                    0                0\n",
            "16   0.05            0         minor                    0                0\n",
            "17   0.75            1         minor                    1                1\n",
            "18   0.95            1         major                    1                1\n",
            "19   0.33            1         mimic                    0                0\n",
            "0.55\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "score_df1, threshold_recall, threshold_pred = evaluate_prediction(score_df)\n",
        "\n",
        "print(\"\\nModified DataFrame:\")\n",
        "print(score_df1)\n",
        "print(threshold_pred)\n",
        "print(threshold_recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rfzkm0kN3uHd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_major_minor_prediction(score_df):\n",
        "    print(\"Evaluating performance on signal vs non-signal hybrids\")\n",
        "\n",
        "    major_hybrid_df = score_df.loc[(score_df[\"ssp_indicator\"] == \"major\") & (score_df[\"hybrid_stat\"] == 1)].copy()\n",
        "    minor_hybrid_df = score_df.loc[(score_df[\"ssp_indicator\"] == \"minor\") & (score_df[\"hybrid_stat\"] == 1)].copy()\n",
        "\n",
        "    major_recall = recall_score(major_hybrid_df[\"hybrid_stat\"], major_hybrid_df[\"converted_preds\"])\n",
        "    minor_recall = recall_score(minor_hybrid_df[\"hybrid_stat\"], minor_hybrid_df[\"converted_preds\"])\n",
        "\n",
        "    major_df = score_df.loc[score_df[\"ssp_indicator\"] == \"major\"].copy()\n",
        "    minor_df = score_df.loc[score_df[\"ssp_indicator\"] == \"minor\"].copy()\n",
        "\n",
        "    major_roc_auc = roc_auc_score(major_df[\"hybrid_stat\"], major_df[\"preds\"])\n",
        "    minor_roc_auc = roc_auc_score(minor_df[\"hybrid_stat\"], minor_df[\"preds\"])\n",
        "\n",
        "    major_prc_auc = average_precision_score(major_df[\"hybrid_stat\"], major_df[\"preds\"])\n",
        "    minor_prc_auc = average_precision_score(minor_df[\"hybrid_stat\"], minor_df[\"preds\"])\n",
        "\n",
        "    print(\"\\nmajor_hybrid_df:\")\n",
        "    print(major_hybrid_df)\n",
        "\n",
        "    print(\"\\nminor_hybrid_df:\")\n",
        "    print(minor_hybrid_df)\n",
        "\n",
        "    print(\"\\nmajor_df:\")\n",
        "    print(major_df)\n",
        "\n",
        "    print(\"\\nminor_df:\")\n",
        "    print(minor_df)\n",
        "\n",
        "    scores = {\n",
        "        \"major_recall\" : major_recall,\n",
        "        \"minor_recall\" : minor_recall,\n",
        "        \"major_prc_auc\" : major_prc_auc,\n",
        "        \"minor_prc_auc\" : minor_prc_auc,\n",
        "        \"major_roc_auc\" : major_roc_auc,\n",
        "        \"minor_roc_auc\" : minor_roc_auc,\n",
        "    }\n",
        "\n",
        "    return scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuS-wjjn_ud_",
        "outputId": "c9454977-e96e-4495-b78d-dd80f326265a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating performance on signal vs non-signal hybrids\n",
            "\n",
            "major_hybrid_df:\n",
            "    preds  hybrid_stat ssp_indicator  pre_converted_preds  converted_preds\n",
            "18   0.95            1         major                    1                1\n",
            "\n",
            "minor_hybrid_df:\n",
            "    preds  hybrid_stat ssp_indicator  pre_converted_preds  converted_preds\n",
            "3    0.80            1         minor                    1                1\n",
            "8    0.70            1         minor                    1                1\n",
            "14   0.65            1         minor                    1                1\n",
            "17   0.75            1         minor                    1                1\n",
            "\n",
            "major_df:\n",
            "    preds  hybrid_stat ssp_indicator  pre_converted_preds  converted_preds\n",
            "0    0.10            0         major                    0                0\n",
            "1    0.40            0         major                    0                0\n",
            "4    0.50            0         major                    0                0\n",
            "9    0.15            0         major                    0                0\n",
            "11   0.25            0         major                    0                0\n",
            "15   0.45            0         major                    0                0\n",
            "18   0.95            1         major                    1                1\n",
            "\n",
            "minor_df:\n",
            "    preds  hybrid_stat ssp_indicator  pre_converted_preds  converted_preds\n",
            "3    0.80            1         minor                    1                1\n",
            "6    0.20            0         minor                    0                0\n",
            "8    0.70            1         minor                    1                1\n",
            "12   0.55            0         minor                    0                0\n",
            "14   0.65            1         minor                    1                1\n",
            "16   0.05            0         minor                    0                0\n",
            "17   0.75            1         minor                    1                1\n",
            "\n",
            "Scores:\n",
            "major_recall: 1.0\n",
            "minor_recall: 1.0\n",
            "major_prc_auc: 1.0\n",
            "minor_prc_auc: 1.0\n",
            "major_roc_auc: 1.0\n",
            "minor_roc_auc: 1.0\n"
          ]
        }
      ],
      "source": [
        "scores = evaluate_major_minor_prediction(score_df1)\n",
        "print(\"\\nScores:\")\n",
        "for key, value in scores.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NDFjFayJYLk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f2-F5bak3wAu"
      },
      "outputs": [],
      "source": [
        "def score_predictions(score_df, mm_vals=False):\n",
        "\n",
        "    score_df, threshold_recall, threshold_pred = evaluate_prediction(score_df)\n",
        "    gt = score_df[\"hybrid_stat\"]\n",
        "    preds = score_df[\"converted_preds\"]\n",
        "\n",
        "    # metrics for hybrids, hybrids are positive here\n",
        "    h_recall = recall_score(gt, preds, pos_label=1)\n",
        "    h_precision = precision_score(gt, preds, pos_label=1)\n",
        "    f1 = f1_score(gt, preds, pos_label=1)\n",
        "    acc = accuracy_score(gt, preds)\n",
        "    prc_auc = average_precision_score(gt, score_df[\"preds\"], pos_label=1) #better when imbalanced class, focus on positive rare; average_precision_score approximates the AUC by a sum over the precisions at every possible threshold value, better than Trapezoidal Rule when the curve has significant non-linearities\n",
        "    roc_auc = roc_auc_score(gt, score_df[\"preds\"])\n",
        "\n",
        "    scores = {\n",
        "        \"threshold_recall\" : float(round(threshold_recall, 4)),\n",
        "        \"threshold_pred\" : float(threshold_pred),\n",
        "        \"hybrid_recall\" : float(h_recall),\n",
        "        \"hybrid_precision\" : float(h_precision),\n",
        "        \"f1_score\" : float(f1),\n",
        "        \"accuracy\" : float(acc),\n",
        "        \"prc_auc\" : float(prc_auc),\n",
        "        \"roc_auc\" : float(roc_auc)\n",
        "    }\n",
        "\n",
        "    if mm_vals:\n",
        "        mm_scores = evaluate_major_minor_prediction(score_df)\n",
        "        scores.update(mm_scores)\n",
        "        print(f\"Full Scores Species A hybrid detection: {scores}\")\n",
        "    else:\n",
        "        print(f\"Full Scores Mimic hybrid detection: {scores}\")\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzDlxSuhSbse",
        "outputId": "25a64580-940b-4426-c07b-d910cec41c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold: 0.05, Recall: 0.1111111111111111\n",
            "Threshold: 0.1, Recall: 0.2222222222222222\n",
            "Threshold: 0.15, Recall: 0.3333333333333333\n",
            "Threshold: 0.2, Recall: 0.4444444444444444\n",
            "Threshold: 0.25, Recall: 0.5555555555555556\n",
            "Threshold: 0.3, Recall: 0.5555555555555556\n",
            "Threshold: 0.33, Recall: 0.5555555555555556\n",
            "Threshold: 0.35, Recall: 0.5555555555555556\n",
            "Threshold: 0.4, Recall: 0.6666666666666666\n",
            "Threshold: 0.45, Recall: 0.7777777777777778\n",
            "Threshold: 0.5, Recall: 0.8888888888888888\n",
            "Threshold: 0.55, Recall: 1.0\n",
            "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0.55 are all non-hybrids and the ones higher are all hybrids.\n",
            "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.55, 'hybrid_recall': 0.7272727272727273, 'hybrid_precision': 1.0, 'f1_score': 0.8421052631578947, 'accuracy': 0.85, 'prc_auc': 0.9218115218115219, 'roc_auc': 0.8787878787878788}\n",
            "{'threshold_recall': 1.0, 'threshold_pred': 0.55, 'hybrid_recall': 0.7272727272727273, 'hybrid_precision': 1.0, 'f1_score': 0.8421052631578947, 'accuracy': 0.85, 'prc_auc': 0.9218115218115219, 'roc_auc': 0.8787878787878788}\n"
          ]
        }
      ],
      "source": [
        "x = score_predictions(score_df1)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFWBCVnTRsq"
      },
      "source": [
        "hand calculated result:\t•\tRecall: 0.727\n",
        "\t•\tPrecision: 1.0\n",
        "\t•\tF1 Score: 0.842\n",
        "\t•\tAccuracy: 0.85\n",
        "\t•\tROC AUC: 0.879\n",
        "\t•\tPRC AUC: 0.922\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TTFsuIJjU8we"
      },
      "outputs": [],
      "source": [
        "pred_df = pd.DataFrame({\n",
        "    \"filename\": [\"file1\", \"file2\", \"file3\", \"file4\"],\n",
        "    \"preds\": [0.8, 0.4, 0.9, 0.2]\n",
        "})\n",
        "sol_df = pd.DataFrame({\n",
        "    \"filename\": [\"file1\", \"file2\", \"file3\", \"file4\"],\n",
        "    \"hybrid_stat\": [1, 0, 1, 0],\n",
        "    \"ssp_indicator\": [\"major\", \"minor\", \"major\", \"mimic\"]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jMOdBxBY3y30"
      },
      "outputs": [],
      "source": [
        "def get_scores(pred_df=None, sol_df=None, mm_vals = False):\n",
        "    # merge ref with predictions\n",
        "    # aligns the ref values with scores in the columns based on filenames\n",
        "    score_df = pd.merge(sol_df, pred_df, on = \"filename\", how = \"inner\")\n",
        "    print(\"score_df:\")\n",
        "    print(score_df)\n",
        "\n",
        "    # Check aligned on all expected files\n",
        "    if score_df.shape[0] != sol_df.shape[0]:\n",
        "        sys.exit(f\"There should have been {sol_df.shape[0]} predictions, but we only got {score_df.shape[0]}\")\n",
        "\n",
        "    scores = score_predictions(score_df, mm_vals) #, config.reverse_score_prediction)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGCerXOSU_3S",
        "outputId": "6e2e3e92-b7b3-40f4-f0d6-6958a43c8359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score_df:\n",
            "  filename  hybrid_stat ssp_indicator  preds\n",
            "0    file1            1         major    0.8\n",
            "1    file2            0         minor    0.4\n",
            "2    file3            1         major    0.9\n",
            "3    file4            0         mimic    0.2\n",
            "Threshold: 0.2, Recall: 0.5\n",
            "Threshold: 0.4, Recall: 1.0\n",
            "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0.4 are all non-hybrids and the ones higher are all hybrids.\n",
            "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.4, 'hybrid_recall': 1.0, 'hybrid_precision': 1.0, 'f1_score': 1.0, 'accuracy': 1.0, 'prc_auc': 1.0, 'roc_auc': 1.0}\n",
            "{'threshold_recall': 1.0, 'threshold_pred': 0.4, 'hybrid_recall': 1.0, 'hybrid_precision': 1.0, 'f1_score': 1.0, 'accuracy': 1.0, 'prc_auc': 1.0, 'roc_auc': 1.0}\n"
          ]
        }
      ],
      "source": [
        "print(get_scores(pred_df, sol_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-ccPCWSQ31-C"
      },
      "outputs": [],
      "source": [
        "def save_scores(path, A_scores, mimic_scores):\n",
        "    score_record = {\n",
        "        \"A_score_major_recall\": A_scores[\"major_recall\"],\n",
        "        \"A_score_minor_recall\": A_scores[\"minor_recall\"],\n",
        "        \"A_PRC_AUC\": A_scores[\"prc_auc\"],\n",
        "        \"A_PRC_AUC_major\": A_scores[\"major_prc_auc\"],\n",
        "        \"A_PRC_AUC_minor\": A_scores[\"minor_prc_auc\"],\n",
        "        \"mimic_recall\": mimic_scores[\"hybrid_recall\"],\n",
        "        \"mimic_PRC_AUC\": mimic_scores[\"prc_auc\"]\n",
        "    }\n",
        "    print(f\"Defined score record for leaderboard {score_record}\")\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(json.dumps(score_record))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tekpzUm5ty_",
        "outputId": "f9919f97-ff5f-4b01-a849-158d9ae82118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using input_dir: /Users/zihengzhang/pipeline_tests/\n",
            "Using solutions: /Users/zihengzhang/pipeline_tests/ref\n",
            "Using prediction_dir: /Users/zihengzhang/pipeline_tests/res\n",
            "Using output_dir: /Users/zihengzhang/pipeline_tests/output\n",
            "files in /Users/zihengzhang/pipeline_tests/ref: ['Dummy_Ref_Data.csv']\n",
            "got solutions with Dummy_Ref_Data.csv\n",
            "score_df:\n",
            "   filename  hybrid_stat ssp_indicator     preds\n",
            "0     file5            0         mimic  0.160740\n",
            "1     file6            1         mimic  0.435405\n",
            "2     file7            0         mimic  0.266075\n",
            "3    file10            1         mimic  0.261777\n",
            "4    file12            0         mimic  0.462449\n",
            "5    file14            1         mimic  0.927865\n",
            "6    file16            0         mimic  0.227994\n",
            "7    file19            0         mimic  0.672110\n",
            "8    file20            0         mimic  0.886525\n",
            "9    file21            0         mimic  0.048348\n",
            "10   file23            0         mimic  0.359483\n",
            "11   file30            1         mimic  0.735290\n",
            "12   file33            1         mimic  0.141647\n",
            "13   file35            0         mimic  0.671493\n",
            "14   file36            1         mimic  0.918017\n",
            "15   file43            1         mimic  0.820505\n",
            "16   file46            0         mimic  0.530700\n",
            "17   file49            1         mimic  0.901932\n",
            "Threshold: 0.0483481659027834, Recall: 0.1\n",
            "Threshold: 0.141647035082454, Recall: 0.1\n",
            "Threshold: 0.160740010650359, Recall: 0.2\n",
            "Threshold: 0.227993866297147, Recall: 0.3\n",
            "Threshold: 0.2617771537491101, Recall: 0.3\n",
            "Threshold: 0.2660747405485247, Recall: 0.4\n",
            "Threshold: 0.3594831387531997, Recall: 0.5\n",
            "Threshold: 0.4354046449815394, Recall: 0.5\n",
            "Threshold: 0.4624485130240722, Recall: 0.6\n",
            "Threshold: 0.5307000864250676, Recall: 0.7\n",
            "Threshold: 0.671492546401421, Recall: 0.8\n",
            "Threshold: 0.6721096555421849, Recall: 0.9\n",
            "Threshold: 0.7352904703087947, Recall: 0.9\n",
            "Threshold: 0.8205051965919047, Recall: 0.9\n",
            "Threshold: 0.8865245899899499, Recall: 1.0\n",
            "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0.8865245899899499 are all non-hybrids and the ones higher are all hybrids.\n",
            "Full Scores Mimic hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.8865245899899499, 'hybrid_recall': 0.375, 'hybrid_precision': 1.0, 'f1_score': 0.5454545454545454, 'accuracy': 0.7222222222222222, 'prc_auc': 0.7686720142602496, 'roc_auc': 0.7125}\n",
            "score_df:\n",
            "   filename  hybrid_stat ssp_indicator     preds\n",
            "0     file1            0         major  0.045757\n",
            "1     file2            0         minor  0.592722\n",
            "2     file3            1         minor  0.066285\n",
            "3     file4            0         minor  0.651092\n",
            "4     file8            0         minor  0.018814\n",
            "5     file9            1         major  0.519999\n",
            "6    file11            1         major  0.534715\n",
            "7    file13            0         minor  0.075784\n",
            "8    file15            1         minor  0.225476\n",
            "9    file17            0         major  0.931408\n",
            "10   file18            0         major  0.369590\n",
            "11   file22            0         minor  0.124446\n",
            "12   file24            0         major  0.588912\n",
            "13   file25            0         major  0.574474\n",
            "14   file26            1         major  0.568597\n",
            "15   file27            1         minor  0.055656\n",
            "16   file28            0         major  0.565356\n",
            "17   file29            0         minor  0.309243\n",
            "18   file31            0         major  0.620914\n",
            "19   file32            0         major  0.935577\n",
            "20   file34            1         major  0.988055\n",
            "21   file37            0         minor  0.201622\n",
            "22   file38            1         major  0.238601\n",
            "23   file39            1         minor  0.988174\n",
            "24   file40            1         minor  0.502642\n",
            "25   file41            0         minor  0.466886\n",
            "26   file42            1         minor  0.105938\n",
            "27   file44            1         minor  0.981482\n",
            "28   file45            0         major  0.782042\n",
            "29   file47            1         major  0.412495\n",
            "30   file48            1         minor  0.205955\n",
            "31   file50            0         major  0.756103\n",
            "Threshold: 0.0188142218951794, Recall: 0.05555555555555555\n",
            "Threshold: 0.0457572590467848, Recall: 0.1111111111111111\n",
            "Threshold: 0.0556558412042331, Recall: 0.1111111111111111\n",
            "Threshold: 0.0662854760558939, Recall: 0.1111111111111111\n",
            "Threshold: 0.0757837176181099, Recall: 0.16666666666666666\n",
            "Threshold: 0.105938054918531, Recall: 0.16666666666666666\n",
            "Threshold: 0.124445844555554, Recall: 0.2222222222222222\n",
            "Threshold: 0.2016216331756227, Recall: 0.2777777777777778\n",
            "Threshold: 0.2059547852012886, Recall: 0.2777777777777778\n",
            "Threshold: 0.2254764214426192, Recall: 0.2777777777777778\n",
            "Threshold: 0.2386013678804966, Recall: 0.2777777777777778\n",
            "Threshold: 0.3092433569993984, Recall: 0.3333333333333333\n",
            "Threshold: 0.3695900875124018, Recall: 0.3888888888888889\n",
            "Threshold: 0.4124951581748732, Recall: 0.3888888888888889\n",
            "Threshold: 0.4668860585466294, Recall: 0.4444444444444444\n",
            "Threshold: 0.5026423361292675, Recall: 0.4444444444444444\n",
            "Threshold: 0.5199985227990187, Recall: 0.4444444444444444\n",
            "Threshold: 0.5347149185555811, Recall: 0.4444444444444444\n",
            "Threshold: 0.5653558158076246, Recall: 0.5\n",
            "Threshold: 0.5685969824597953, Recall: 0.5\n",
            "Threshold: 0.5744740056805542, Recall: 0.5555555555555556\n",
            "Threshold: 0.5889122374984956, Recall: 0.6111111111111112\n",
            "Threshold: 0.5927224554225187, Recall: 0.6666666666666666\n",
            "Threshold: 0.6209137342084907, Recall: 0.7222222222222222\n",
            "Threshold: 0.6510921957270309, Recall: 0.7777777777777778\n",
            "Threshold: 0.7561026624364624, Recall: 0.8333333333333334\n",
            "Threshold: 0.7820418452590028, Recall: 0.8888888888888888\n",
            "Threshold: 0.931407551499328, Recall: 0.9444444444444444\n",
            "Threshold: 0.935576766212736, Recall: 1.0\n",
            "With non-hybrid recall 1.0, the predictions equal and lower than the threshold confident score 0.935576766212736 are all non-hybrids and the ones higher are all hybrids.\n",
            "Evaluating performance on signal vs non-signal hybrids\n",
            "\n",
            "major_hybrid_df:\n",
            "   filename  hybrid_stat ssp_indicator     preds  converted_preds\n",
            "5     file9            1         major  0.519999                0\n",
            "6    file11            1         major  0.534715                0\n",
            "14   file26            1         major  0.568597                0\n",
            "20   file34            1         major  0.988055                1\n",
            "22   file38            1         major  0.238601                0\n",
            "29   file47            1         major  0.412495                0\n",
            "\n",
            "minor_hybrid_df:\n",
            "   filename  hybrid_stat ssp_indicator     preds  converted_preds\n",
            "2     file3            1         minor  0.066285                0\n",
            "8    file15            1         minor  0.225476                0\n",
            "15   file27            1         minor  0.055656                0\n",
            "23   file39            1         minor  0.988174                1\n",
            "24   file40            1         minor  0.502642                0\n",
            "26   file42            1         minor  0.105938                0\n",
            "27   file44            1         minor  0.981482                1\n",
            "30   file48            1         minor  0.205955                0\n",
            "\n",
            "major_df:\n",
            "   filename  hybrid_stat ssp_indicator     preds  converted_preds\n",
            "0     file1            0         major  0.045757                0\n",
            "5     file9            1         major  0.519999                0\n",
            "6    file11            1         major  0.534715                0\n",
            "9    file17            0         major  0.931408                0\n",
            "10   file18            0         major  0.369590                0\n",
            "12   file24            0         major  0.588912                0\n",
            "13   file25            0         major  0.574474                0\n",
            "14   file26            1         major  0.568597                0\n",
            "16   file28            0         major  0.565356                0\n",
            "18   file31            0         major  0.620914                0\n",
            "19   file32            0         major  0.935577                0\n",
            "20   file34            1         major  0.988055                1\n",
            "22   file38            1         major  0.238601                0\n",
            "28   file45            0         major  0.782042                0\n",
            "29   file47            1         major  0.412495                0\n",
            "31   file50            0         major  0.756103                0\n",
            "\n",
            "minor_df:\n",
            "   filename  hybrid_stat ssp_indicator     preds  converted_preds\n",
            "1     file2            0         minor  0.592722                0\n",
            "2     file3            1         minor  0.066285                0\n",
            "3     file4            0         minor  0.651092                0\n",
            "4     file8            0         minor  0.018814                0\n",
            "7    file13            0         minor  0.075784                0\n",
            "8    file15            1         minor  0.225476                0\n",
            "11   file22            0         minor  0.124446                0\n",
            "15   file27            1         minor  0.055656                0\n",
            "17   file29            0         minor  0.309243                0\n",
            "21   file37            0         minor  0.201622                0\n",
            "23   file39            1         minor  0.988174                1\n",
            "24   file40            1         minor  0.502642                0\n",
            "25   file41            0         minor  0.466886                0\n",
            "26   file42            1         minor  0.105938                0\n",
            "27   file44            1         minor  0.981482                1\n",
            "30   file48            1         minor  0.205955                0\n",
            "Full Scores Species A hybrid detection: {'threshold_recall': 1.0, 'threshold_pred': 0.935576766212736, 'hybrid_recall': 0.21428571428571427, 'hybrid_precision': 1.0, 'f1_score': 0.35294117647058826, 'accuracy': 0.65625, 'prc_auc': 0.5364597716276366, 'roc_auc': 0.4603174603174603, 'major_recall': 0.16666666666666666, 'minor_recall': 0.25, 'major_prc_auc': 0.4354830354830355, 'minor_prc_auc': 0.648611111111111, 'major_roc_auc': 0.3333333333333333, 'minor_roc_auc': 0.53125}\n",
            "Defined score record for leaderboard {'A_score_major_recall': 0.16666666666666666, 'A_score_minor_recall': 0.25, 'A_PRC_AUC': 0.5364597716276366, 'A_PRC_AUC_major': 0.4354830354830355, 'A_PRC_AUC_minor': 0.648611111111111, 'mimic_recall': 0.375, 'mimic_PRC_AUC': 0.7686720142602496}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "def main(input_dir, output_dir):\n",
        "    # Directory to read labels from\n",
        "    solutions = os.path.join(input_dir, 'ref')\n",
        "    prediction_dir = os.path.join(input_dir, 'res')\n",
        "\n",
        "    print(\"Using input_dir: \" + input_dir)\n",
        "    print(\"Using solutions: \" + solutions)\n",
        "    print(\"Using prediction_dir: \" + prediction_dir)\n",
        "    print(\"Using output_dir: \" + output_dir)\n",
        "\n",
        "    prediction_file = os.path.join(prediction_dir,'/Users/zihengzhang/pipeline_tests/res/Dummy_Prediction_Data.txt')\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.isfile(prediction_file):\n",
        "        print('[-] Test prediction file not found!')\n",
        "        print(prediction_file)\n",
        "        sys.exit(\"Couldn't read predictions\")\n",
        "\n",
        "    # Get predictions\n",
        "    pred_df = pd.read_csv(prediction_file, header=None, names=['filename','preds'])\n",
        "\n",
        "    # Get solutions\n",
        "    file_list = os.listdir(solutions)\n",
        "    print(f\"files in {solutions}: {file_list}\")\n",
        "\n",
        "    # Check if file exists --- should generally be file_list[0], there's a '__MACOSX' showing up from zipping\n",
        "    if \"ref_val.csv\" in file_list:\n",
        "        solution_file = os.path.join(solutions,'ref_val.csv')\n",
        "        sol_df_A, sol_df_mimic = parse_solution_file(solution_file)\n",
        "        print(\"got solutions with 'ref_val.csv'\")\n",
        "    elif \"ref_test.csv\" in file_list:\n",
        "        solution_file = os.path.join(solutions,'ref_test.csv')\n",
        "        sol_df_A, sol_df_mimic = parse_solution_file(solution_file)\n",
        "        print(\"got solutions with 'ref_test.csv'\")\n",
        "    elif len(file_list) > 0:\n",
        "        solution_file = os.path.join(solutions, file_list[0])\n",
        "        sol_df_A, sol_df_mimic = parse_solution_file(solution_file)\n",
        "        print(f\"got solutions with {file_list[0]}\")\n",
        "    else:\n",
        "        sys.exit(f\"Couldn't find solution file, have {len(file_list)} files in {solutions}\")\n",
        "\n",
        "    mimic_scores = get_scores(pred_df=pred_df, sol_df=sol_df_mimic, mm_vals = False)\n",
        "    A_scores = get_scores(pred_df=pred_df, sol_df=sol_df_A, mm_vals = True)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(pathlib.Path(output_dir).parent.resolve(), exist_ok=True)\n",
        "    output_file = os.path.join(output_dir, \"scores.json\")\n",
        "\n",
        "    # Write scores\n",
        "    save_scores(output_file, A_scores, mimic_scores)\n",
        "\n",
        "\n",
        "input_dir = '/Users/zihengzhang/pipeline_tests/'\n",
        "output_dir = '/Users/zihengzhang/pipeline_tests/output'\n",
        "\n",
        "main(input_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
